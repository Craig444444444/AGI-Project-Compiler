#!/usr/bin/env python3
"""
Enhanced AGI Project Compiler with Multi-LLM Knowledge Augmentation
Production-Grade Implementation Incorporating All Recommendations """

import os
import json
import re
import time
import asyncio
import hashlib
import logging
import pathlib
import functools
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple, Set
from collections import deque

import torch
import aiohttp
import numpy as np
from tqdm import tqdm
from pydantic import BaseModel, ValidationError
from sentence_transformers import SentenceTransformer
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    AutoModelForSequenceClassification
)

# -----------------------
# CONFIGURATION (Enhanced)
# -----------------------

class CompilerConfig(BaseModel):
    project_path: str = "./sample_project"
    output_dir: str = "./output"
    cache_dir: str = "./cache"
    log_dir: str = "./logs"
    vector_db_dir: str = "./vector_db"
    text_extensions: List[str] = [".py", ".md", ".txt", ".js", ".html", ".css"]
    exclude_dirs: List[str] = [".git", ".venv", "__pycache__", "node_modules"]
    exclude_files: List[str] = [".DS_Store", "thumbs.db"]
    candidate_labels: List[str] = [
        "web scraping", "database operations", "API endpoint definition",
        "utility function", "configuration", "frontend component"
    ]
    research_depth: int = 1
    max_concurrent_requests: int = 10  # Updated for async
    chunk_size: int = 1024
    max_file_size: int = 2 * 1024 * 1024  # 2MB
    serpapi_key: Optional[str] = None
    serpapi_monthly_volume: int = 1000
    use_web_search: bool = True
    use_local_knowledge: bool = True
    knowledge_cache: bool = True
    cache_ttl_hours: int = 24
    structured_output: bool = True
    max_retries: int = 3
    retry_delay: float = 2.0
    token_limit: int = 4096
    token_safety_margin: int = 200
    default_temperature: float = 0.7
    deterministic_temperature: float = 0.0
    semantic_cache_threshold: float = 0.85  # Similarity threshold
    rate_limit_rpm: int = 300  # Requests per minute
    circuit_breaker_threshold: int = 5  # Failures before tripping

# -----------------------
# LOGGER SETUP (Enhanced)
# -----------------------

def setup_logger(config: CompilerConfig) -> logging.Logger:
    """Configure comprehensive logging system with file rotation"""
    os.makedirs(config.log_dir, exist_ok=True)
    log_file = os.path.join(
        config.log_dir,
        f"compiler_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    )
    
    logger = logging.getLogger("AGI_Compiler")
    logger.setLevel(logging.DEBUG)
    
    # File handler with rotation
    fh = logging.FileHandler(log_file)
    fh.setLevel(logging.DEBUG)
    
    # Console handler
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    
    formatter = logging.Formatter(
        '[%(asctime)s] [%(levelname)s] [%(module)s:%(lineno)d] - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    fh.setFormatter(formatter)
    ch.setFormatter(formatter)
    
    logger.addHandler(fh)
    logger.addHandler(ch)
    
    return logger

# -----------------------
# STRUCTURED OUTPUT SCHEMAS
# -----------------------

class KnowledgeGapResponse(BaseModel):
    gaps: List[str]
    confidence: float

class SearchQueryResponse(BaseModel):
    queries: List[str]
    reasoning: str

class ResearchResult(BaseModel):
    source: str
    title: str
    content: str
    url: str
    relevance_score: float

# -----------------------
# CIRCUIT BREAKER PATTERN
# -----------------------

class CircuitBreaker:
    def __init__(self, threshold: int, reset_timeout: int = 60):
        self.threshold = threshold
        self.reset_timeout = reset_timeout
        self.failures = 0
        self.last_failure = None
        self.tripped = False

    def record_failure(self):
        self.failures += 1
        self.last_failure = time.time()
        if self.failures >= self.threshold:
            self.tripped = True

    def record_success(self):
        self.failures = 0
        self.tripped = False

    def is_tripped(self):
        if self.tripped and (time.time() - self.last_failure) > self.reset_timeout:
            self.tripped = False
            self.failures = 0
        return self.tripped

# -----------------------
# RATE LIMITER (Token Bucket)
# -----------------------

class RateLimiter:
    def __init__(self, rpm: int):
        self.rate = rpm / 60  # Requests per second
        self.tokens = rpm
        self.max_tokens = rpm
        self.last_update = time.time()
        self.lock = asyncio.Lock()

    async def wait_for_token(self):
        async with self.lock:
            now = time.time()
            elapsed = now - self.last_update
            self.last_update = now
            
            # Add new tokens
            self.tokens = min(
                self.max_tokens,
                self.tokens + elapsed * self.rate
            )
            
            if self.tokens >= 1:
                self.tokens -= 1
                return
            
            # Calculate wait time
            deficit = 1 - self.tokens
            wait_time = deficit / self.rate
            await asyncio.sleep(wait_time)
            self.tokens = 0

# -----------------------
# REQUEST COALESCING
# -----------------------

class RequestCoalescer:
    def __init__(self):
        self.active_requests = {}
        self.lock = asyncio.Lock()

    async def coalesce(self, key, coro):
        async with self.lock:
            if key in self.active_requests:
                return await self.active_requests[key]
            
            task = asyncio.create_task(coro)
            self.active_requests[key] = task
            try:
                result = await task
                return result
            finally:
                async with self.lock:
                    del self.active_requests[key]

# -----------------------
# HARDWARE-AWARE MODEL LOADER (Enhanced)
# -----------------------

class HardwareAwareModelLoader:
    """Optimized model loader with hardware awareness and fallback"""
    def __init__(self, config: CompilerConfig, logger: logging.Logger):
        self.config = config
        self.logger = logger
        self.device = self._detect_device()
        self.model_config = self._get_hardware_config()
        
        # Ensure cache directory exists
        os.makedirs(config.cache_dir, exist_ok=True)
        
    def _detect_device(self) -> str:
        """Detect available hardware with CUDA support check"""
        if torch.cuda.is_available():
            gpu_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            self.logger.info(f"Detected CUDA device with {gpu_mem:.1f}GB memory")
            return "cuda"
        return "cpu"
    
    def _get_hardware_config(self) -> Dict[str, Tuple[str, str]]:
        """Select optimal models based on available hardware"""
        if self.device == "cuda":
            gpu_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            if gpu_mem > 40:  # A100/H100 class
                return {
                    "large": ("tiiuae/falcon-7b-instruct", "4bit"),
                    "small": ("gpt2", "fp32"),
                    "classifier": ("facebook/bart-large-mnli", "fp16"),
                    "embedding": ("sentence-transformers/all-MiniLM-L6-v2", "fp32")
                }
            elif gpu_mem > 24:  # RTX 3090/4090 class
                return {
                    "large": ("facebook/opt-6.7b", "8bit"),
                    "small": ("distilgpt2", "fp32"),
                    "classifier": ("typeform/distilbert-base-uncased-mnli", "fp32"),
                    "embedding": ("sentence-transformers/all-MiniLM-L6-v2", "fp32")
                }
            else:  # Lower-end GPUs
                return {
                    "large": ("facebook/opt-1.3b", "fp32"),
                    "small": ("distilgpt2", "fp32"),
                    "classifier": ("typeform/distilbert-base-uncased-mnli", "fp32"),
                    "embedding": ("sentence-transformers/all-MiniLM-L6-v2", "fp32")
                }
        return {  # CPU-only fallback
            "large": ("distilgpt2", "fp32"),
            "small": ("distilgpt2", "fp32"),
            "classifier": ("typeform/distilbert-base-uncased-mnli", "fp32"),
            "embedding": ("sentence-transformers/all-MiniLM-L6-v2", "fp32")
        }
    
    def _get_quant_config(self, precision: str) -> Optional[BitsAndBytesConfig]:
        """Get quantization configuration based on precision"""
        if precision == "4bit":
            return BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_quant_type="nf4"
            )
        elif precision == "8bit":
            return BitsAndBytesConfig(load_in_8bit=True)
        return None
    
    async def load_models(self) -> Dict[str, Tuple[Any, Any]]:
        """Load all required models with fallback handling"""
        models = {}
        for model_type in ["large", "small", "classifier", "embedding"]:
            try:
                model_name, precision = self.model_config[model_type]
                self.logger.info(f"Loading {model_type} model: {model_name} ({precision})")
                
                quant_config = self._get_quant_config(precision)
                torch_dtype = torch.float16 if precision in ["4bit", "fp16"] else torch.float32
                
                if model_type == "classifier":
                    model = AutoModelForSequenceClassification.from_pretrained(
                        model_name,
                        quantization_config=quant_config,
                        device_map="auto",
                        torch_dtype=torch_dtype,
                        trust_remote_code=True,
                        cache_dir=self.config.cache_dir
                    )
                elif model_type == "embedding":
                    model = SentenceTransformer(
                        model_name,
                        device=self.device,
                        cache_folder=self.config.cache_dir
                    )
                else:
                    model = AutoModelForCausalLM.from_pretrained(
                        model_name,
                        quantization_config=quant_config,
                        device_map="auto",
                        torch_dtype=torch_dtype,
                        trust_remote_code=True,
                        cache_dir=self.config.cache_dir
                    )
                
                if model_type != "embedding":
                    tokenizer = AutoTokenizer.from_pretrained(
                        model_name,
                        cache_dir=self.config.cache_dir
                    )
                    models[model_type] = (model, tokenizer)
                else:
                    models[model_type] = (model, None)
                
            except Exception as e:
                self.logger.error(f"Failed to load {model_type} model: {str(e)}")
                raise RuntimeError(f"Critical model loading failure for {model_type}")
        
        return models

# -----------------------
# KNOWLEDGE ENGINE (Production-Grade)
# -----------------------

class HybridKnowledgeEngine:
    """Robust knowledge engine with caching, TTL, and multi-source retrieval"""
    def __init__(self, config: CompilerConfig, logger: logging.Logger):
        self.config = config
        self.logger = logger
        self.cache = {}
        self.rate_limiter = RateLimiter(config.rate_limit_rpm)
        self.request_coalescer = RequestCoalescer()
        self.circuit_breaker = CircuitBreaker(config.circuit_breaker_threshold)
        self.session = aiohttp.ClientSession()
        
        # Setup cache and vector DB directories
        os.makedirs(config.cache_dir, exist_ok=True)
        os.makedirs(config.vector_db_dir, exist_ok=True)
        
    async def close(self):
        await self.session.close()
        
    @functools.lru_cache(maxsize=1024)
    def _memory_cache(self, query: str) -> Optional[List[Dict]]:
        """In-memory LRU cache for frequent queries"""
        return None
        
    def _disk_cache_path(self, query: str) -> str:
        """Get disk cache file path for a query"""
        query_hash = hashlib.sha256(query.encode()).hexdigest()
        return os.path.join(self.config.cache_dir, f"knowledge_{query_hash}.json")
    
    def _load_disk_cache(self, query: str) -> Optional[List[Dict]]:
        """Load cache from disk if available and not expired"""
        cache_path = self._disk_cache_path(query)
        
        try:
            with open(cache_path, "r") as f:
                cache_data = json.load(f)
                cache_time = datetime.fromisoformat(cache_data["timestamp"])
                
                if datetime.now() - cache_time < timedelta(hours=self.config.cache_ttl_hours):
                    return cache_data["results"]
        except (FileNotFoundError, json.JSONDecodeError, KeyError) as e:
            self.logger.debug(f"Cache miss for {query}: {str(e)}")
        
        return None
        
    def _save_disk_cache(self, query: str, results: List[Dict]):
        """Save results to disk cache"""
        cache_path = self._disk_cache_path(query)
        cache_data = {
            "timestamp": datetime.now().isoformat(),
            "query": query,
            "results": results
        }
        
        try:
            with open(cache_path, "w") as f:
                json.dump(cache_data, f, indent=2)
        except IOError as e:
            self.logger.error(f"Failed to save cache for {query}: {str(e)}")
    
    async def deep_dive(self, query: str, depth: int = 1) -> List[Dict[str, Any]]:
        """Hybrid research with multi-tier caching and rate limiting"""
        if self.circuit_breaker.is_tripped():
            self.logger.warning("Circuit breaker tripped - skipping research")
            return []
            
        # Check memory cache
        if cached := self._memory_cache(query):
            self.logger.info(f"Using memory cache for: {query}")
            return cached
            
        # Check disk cache
        if cached := self._load_disk_cache(query):
            self.logger.info(f"Using disk cache for: {query}")
            return cached
            
        # Conduct fresh research
        results = []
        retries = 0
        
        while not results and retries < self.config.max_retries:
            try:
                # Web search
                if self.config.use_web_search and self.config.serpapi_key:
                    await self.rate_limiter.wait_for_token()
                    results.extend(await self._serpapi_search(query, depth))
                
                # Local knowledge
                if self.config.use_local_knowledge and not results:
                    results.extend(await self._local_knowledge(query))
                
                # Mock fallback if no results
                if not results:
                    results = await self._mock_research(query)
                    
                self.circuit_breaker.record_success()
                
            except Exception as e:
                self.logger.error(f"Research error (attempt {retries + 1}): {str(e)}")
                self.circuit_breaker.record_failure()
                retries += 1
                await asyncio.sleep(self.config.retry_delay ** retries)
        
        # Save to cache
        if results:
            self._save_disk_cache(query, results)
        
        return results
    
    async def _serpapi_search(self, query: str, depth: int) -> List[Dict]:
        """Search using SerpAPI with enhanced error handling"""
        try:
            url = "https://serpapi.com/search"
            params = {
                "q": query,
                "api_key": self.config.serpapi_key,
                "num": min(10 * depth, 50)  # Scale with depth
            }
            
            async with self.session.get(url, params=params, timeout=30) as response:
                response.raise_for_status()
                data = await response.json()
            
            results = []
            for result in data.get("organic_results", []):
                results.append(ResearchResult(
                    source="serpapi",
                    title=result.get("title", ""),
                    content=result.get("snippet", ""),
                    url=result.get("link", ""),
                    relevance_score=0.8  # Placeholder for actual relevance scoring
                ).dict())
            
            self.logger.info(f"Found {len(results)} web results for: {query}")
            return results
            
        except aiohttp.ClientError as e:
            self.logger.error(f"SerpAPI request failed: {str(e)}")
            return []
    
    async def _local_knowledge(self, query: str) -> List[Dict]:
        """Search local knowledge base (placeholder for implementation)"""
        return []
    
    async def _mock_research(self, query: str) -> List[Dict]:
        """Fallback to mock research"""
        await asyncio.sleep(0.2)
        return [ResearchResult(
            source="mock",
            title=f"Research: {query}",
            content=f"Simulated research result for '{query}'",
            url="",
            relevance_score=0.5
        ).dict()]

# -----------------------
# PROJECT COMPILER (Enhanced)
# -----------------------

class AGIProjectCompiler:
    """Production-grade AGI compiler implementing all recommendations"""
    def __init__(self, config: CompilerConfig):
        self.config = config
        self.logger = setup_logger(config)
        
        # Initialize async components
        self.loop = asyncio.get_event_loop()
        
        # Load models with hardware awareness
        self.model_loader = HardwareAwareModelLoader(config, self.logger)
        self.models = self.loop.run_until_complete(self.model_loader.load_models())
        self.large_llm, self.large_tokenizer = self.models["large"]
        self.small_llm, self.small_tokenizer = self.models["small"]
        self.classifier, self.classifier_tokenizer = self.models["classifier"]
        self.embedding_model, _ = self.models["embedding"]
        
        # Initialize knowledge engine
        self.knowledge_engine = HybridKnowledgeEngine(config, self.logger)
        
        # Project state
        self.project_structure = {}
        self.semantic_network = {}
        self.file_hashes = {}
        self.token_usage = {}
        self.stats = {
            "files_processed": 0,
            "files_augmented": 0,
            "knowledge_gaps": 0,
            "research_queries": 0,
            "cache_hits": 0,
            "api_calls": 0,
            "token_usage": 0,
            "errors": 0,
            "start_time": time.time()
        }

    async def close(self):
        """Cleanup resources"""
        await self.knowledge_engine.close()

    # --------------------------
    # FILE SYSTEM OPERATIONS (Robust)
    # --------------------------
    
    async def load_project_files(self):
        """Load files with comprehensive error handling"""
        self.logger.info("Loading project files from disk...")
        self.project_structure = await self._load_project_files_real()
        total_files = len(self._get_all_files())
        self.logger.info(f"Found {total_files} files in project")
        
        # Classify files with error handling
        for file_data in tqdm(self._get_all_files(), desc="Classifying files"):
            try:
                self.stats["files_processed"] += 1
                file_data["semantic_type"] = await self.classify_file(file_data)
                self.logger.debug(f"Classified {file_data['path']} as {file_data['semantic_type']}")
            except Exception as e:
                self.stats["errors"] += 1
                self.logger.error(f"Classification failed for {file_data['path']}: {str(e)}")
                file_data["semantic_type"] = "unknown"
    
    async def _load_project_files_real(self) -> Dict[str, List[Dict]]:
        """Robust file loader with exclusion handling and size limits"""
        project_path = pathlib.Path(self.config.project_path)
        files_dict = {}
        
        if not project_path.exists():
            raise FileNotFoundError(f"Project path {project_path} does not exist")
        
        exclude_dirs = set(self.config.exclude_dirs)
        exclude_files = set(self.config.exclude_files)
        
        for root, dirs, files in os.walk(project_path, topdown=True):
            # Filter excluded directories
            dirs[:] = [d for d in dirs if d not in exclude_dirs]
            
            for file in files:
                if file in exclude_files:
                    continue
                    
                file_path = pathlib.Path(root) / file
                rel_path = file_path.relative_to(project_path)
                
                # Check file extension
                if file_path.suffix.lower() not in self.config.text_extensions:
                    continue
                
                # Check file size
                try:
                    file_size = file_path.stat().st_size
                    if file_size > self.config.max_file_size:
                        self.logger.warning(f"Skipping large file: {rel_path} ({file_size} bytes)")
                        continue
                except OSError as e:
                    self.logger.error(f"File size check failed: {rel_path} - {str(e)}")
                    continue
                
                # Read file content with proper error handling
                try:
                    # Using surrogateescape for encoding errors
                    with open(file_path, "r", encoding="utf-8", errors="surrogateescape") as f:
                        content = f.read()
                except Exception as e:
                    self.logger.error(f"Failed to read {rel_path}: {str(e)}")
                    continue
                
                # Calculate file hash
                try:
                    file_hash = await self._calculate_file_hash(file_path)
                except Exception as e:
                    self.logger.error(f"Hash calculation failed: {rel_path} - {str(e)}")
                    file_hash = "error"
                
                # Store file data
                module = str(rel_path.parent) if rel_path.parent != pathlib.Path(".") else "root"
                file_data = {
                    "path": str(rel_path),
                    "content": content,
                    "metadata": {
                        "size": file_size,
                        "modified": os.path.getmtime(file_path),
                        "hash": file_hash
                    }
                }
                files_dict.setdefault(module, []).append(file_data)
                self.file_hashes[str(rel_path)] = file_hash
        
        return files_dict
    
    async def _calculate_file_hash(self, file_path: pathlib.Path) -> str:
        """Calculate SHA256 hash with proper resource handling"""
        hasher = hashlib.sha256()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hasher.update(chunk)
        return hasher.hexdigest()

    # --------------------------
    # LLM OPERATIONS (Structured)
    # --------------------------
    
    async def classify_file(self, file_data: Dict[str, Any]) -> str:
        """Classify file using specialized model with token limits"""
        # Truncate to model's max length
        max_length = self.classifier.config.max_position_embeddings
        content = file_data["content"][:min(self.config.chunk_size, max_length)]
        
        inputs = self.classifier_tokenizer(
            content,
            return_tensors="pt",
            truncation=True,
            max_length=max_length
        ).to(self.model_loader.device)

        with torch.no_grad():
            outputs = self.classifier(**inputs)
        
        # Get probabilities
        probs = torch.softmax(outputs.logits, dim=1)[0]
        pred_idx = torch.argmax(probs).item()
        confidence = probs[pred_idx].item()
        
        # Only return if confidence is high
        return self.config.candidate_labels[pred_idx] if confidence > 0.5 else "unknown"
    
    async def analyze_knowledge_gaps(self, file_data: Dict[str, Any]) -> List[str]:
        """Structured knowledge gap analysis with JSON schema"""
        # Prepare prompt with structured output instructions
        prompt = (
            f"Analyze this {file_data['semantic_type']} file and list knowledge gaps:\n"
            f"Path: {file_data['path']}\n"
            f"Content:\n{file_data['content'][:500]}\n\n"
            "Respond with a JSON object containing a 'gaps' list and 'confidence' score.\n"
            "Example: {'gaps': ['gap1', 'gap2'], 'confidence': 0.85}"
        )
        
        # Enforce structured output
        try:
            response = await self._structured_llm_call(
                prompt=prompt,
                model=self.large_llm,
                tokenizer=self.large_tokenizer,
                response_model=KnowledgeGapResponse,
                temperature=self.config.deterministic_temperature,
                max_tokens=200
            )
            if response.confidence < 0.5:
                self.logger.warning(f"Low confidence ({response.confidence}) in gap analysis")
            return response.gaps
        except Exception as e:
            self.logger.error(f"Structured gap analysis failed: {str(e)}")
            return []
    
    async def _structured_llm_call(self, prompt: str, model: Any, tokenizer: Any, 
                                response_model: BaseModel, temperature: float, 
                                max_tokens: int) -> BaseModel:
        """Make LLM call with enforced structured output"""
        inputs = tokenizer(prompt, return_tensors="pt").to(self.model_loader.device)
        input_length = inputs.input_ids.shape[1]
        
        # Check token limits
        if input_length + max_tokens > self.config.token_limit - self.config.token_safety_margin:
            self.logger.warning("Token limit exceeded, truncating input")
            excess = (input_length + max_tokens) - (self.config.token_limit - self.config.token_safety_margin)
            inputs = tokenizer(prompt[:-excess], return_tensors="pt").to(self.model_loader.device)
            input_length = inputs.input_ids.shape[1]
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=temperature,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # Extract and clean response
        response_text = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True)
        cleaned_text = re.sub(r'[^{}]+', '', response_text, 1)  # Find first JSON object
        
        # Parse and validate
        try:
            response_data = json.loads(cleaned_text)
            validated = response_model(**response_data)
            return validated
        except (json.JSONDecodeError, ValidationError) as e:
            self.logger.error(f"Structured output parsing failed: {str(e)}")
            self.logger.debug(f"Original response: {response_text}")
            raise ValueError("Failed to parse structured LLM output")
    
    async def generate_search_queries(self, knowledge_gaps: List[str]) -> List[str]:
        """Structured query generation with JSON schema"""
        prompt = (
            "Convert these knowledge gaps to web search queries:\n"
            f"{', '.join(knowledge_gaps)}\n\n"
            "Respond with a JSON object containing a 'queries' list and 'reasoning'.\n"
            "Example: {'queries': ['query1', 'query2'], 'reasoning': '...'}"
        )
        
        # Enforce structured output
        try:
            response = await self._structured_llm_call(
                prompt=prompt,
                model=self.small_llm,
                tokenizer=self.small_tokenizer,
                response_model=SearchQueryResponse,
                temperature=self.config.deterministic_temperature,
                max_tokens=150
            )
            self.logger.debug(f"Query generation reasoning: {response.reasoning}")
            return response.queries
        except Exception as e:
            self.logger.error(f"Structured query generation failed: {str(e)}")
            return []

    # --------------------------
    # AUGMENTATION PIPELINE (Concurrent)
    # --------------------------
    
    async def augment_file_knowledge(self, file_data: Dict[str, Any]):
        """Robust augmentation pipeline with error handling"""
        try:
            # Skip already augmented files
            if file_data.get("metadata", {}).get("augmented", False):
                return
                
            # Step 1: Identify knowledge gaps
            gaps = await self.analyze_knowledge_gaps(file_data)
            if not gaps:
                self.logger.debug(f"No knowledge gaps found for {file_data['path']}")
                return
            self.stats["knowledge_gaps"] += len(gaps)
            
            # Step 2: Generate research queries
            queries = await self.generate_search_queries(gaps)
            if not queries:
                self.logger.debug(f"No queries generated for {file_data['path']}")
                return
            self.stats["research_queries"] += len(queries)
            
            # Step 3: Conduct research
            research_results = []
            for query in queries:
                results = await self.knowledge_engine.deep_dive(query, self.config.research_depth)
                research_results.extend(results)
            
            # Step 4: Update metadata
            if "metadata" not in file_data:
                file_data["metadata"] = {}
                
            file_data["metadata"].update({
                "augmented": True,
                "augmented_at": time.time(),
                "research_queries": queries,
                "research_results": research_results
            })
            
            self.stats["files_augmented"] += 1
            self.logger.info(f"Augmented file: {file_data['path']}")
            
        except Exception as e:
            self.stats["errors"] += 1
            self.logger.error(f"Error augmenting {file_data['path']}: {str(e)}")
    
    async def run_augmentation(self):
        """Concurrent augmentation with asyncio"""
        files_to_process = [f for f in self._get_all_files() 
                          if not f.get("metadata", {}).get("augmented", False)]
        
        self.logger.info(f"Found {len(files_to_process)} files to augment")
        
        # Create semaphore for concurrency control
        semaphore = asyncio.Semaphore(self.config.max_concurrent_requests)
        
        async def process_file(file_data):
            async with semaphore:
                await self.augment_file_knowledge(file_data)
        
        # Run tasks with progress tracking
        tasks = [process_file(f) for f in files_to_process]
        for f in asyncio.as_completed(tasks):
            try:
                await f
            except Exception as e:
                self.stats["errors"] += 1
                self.logger.error(f"Augmentation task failed: {str(e)}")

    # --------------------------
    # OUTPUT AND UTILITIES
    # --------------------------
    
    async def save_results(self):
        """Save results with comprehensive metadata"""
        os.makedirs(self.config.output_dir, exist_ok=True)
        output_path = os.path.join(self.config.output_dir, "project_data.json")
        
        # Prepare output data
        output_data = {
            "project_structure": self.project_structure,
            "semantic_network": self.semantic_network,
            "file_hashes": self.file_hashes,
            "stats": self.stats,
            "config": self.config.dict(),
            "timestamp": time.time(),
            "duration": time.time() - self.stats["start_time"]
        }
        
        try:
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(output_data, f, indent=2, ensure_ascii=False)
            self.logger.info(f"Results saved to: {output_path}")
        except Exception as e:
            self.logger.error(f"Failed to save results: {str(e)}")
    
    def _get_all_files(self) -> List[Dict]:
        return [f for files in self.project_structure.values() for f in files]

# -----------------------
# MAIN EXECUTION
# -----------------------

async def main():
    # Initialize configuration
    config = CompilerConfig(
        serpapi_key=os.getenv("SERPAPI_KEY"),
        project_path="./my_project",
        output_dir="./compilation_output",
        cache_dir="./knowledge_cache",
        vector_db_dir="./vector_db"
    )
    
    # Create necessary directories
    for dir_path in [config.project_path, config.output_dir, config.cache_dir, 
                    config.log_dir, config.vector_db_dir]:
        os.makedirs(dir_path, exist_ok=True)
    
    # Initialize compiler
    compiler = AGIProjectCompiler(config)
    
    try:
        # Execute compilation pipeline
        await compiler.load_project_files()
        await compiler.run_augmentation()
        await compiler.save_results()
        
        # Print summary
        stats = compiler.stats
        duration = stats.get("duration", time.time() - stats["start_time"])
        logger = compiler.logger
        
        logger.info("\n===== COMPILATION SUMMARY =====")
        logger.info(f"Files processed: {stats['files_processed']}")
        logger.info(f"Files augmented: {stats['files_augmented']}")
        logger.info(f"Knowledge gaps identified: {stats['knowledge_gaps']}")
        logger.info(f"Research queries generated: {stats['research_queries']}")
        logger.info(f"Cache hits: {stats.get('cache_hits', 0)}")
        logger.info(f"API calls: {stats.get('api_calls', 0)}")
        logger.info(f"Token usage: {stats.get('token_usage', 0)}")
        logger.info(f"Errors encountered: {stats['errors']}")
        logger.info(f"Total time: {duration:.2f} seconds")
        logger.info("Compilation complete!")
        
    except Exception as e:
        compiler.logger.critical(f"Critical error during compilation: {str(e)}")
        exit(1)
    finally:
        await compiler.close()

if __name__ == "__main__":
    asyncio.run(main())
